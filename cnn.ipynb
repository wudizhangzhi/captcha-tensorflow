{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moon/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/moon/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "height = 60\n",
    "width = 34\n",
    "LR = 0.001\n",
    "\n",
    "img_input = layers.Input((height, width,  1))\n",
    "\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "# print(x.shape)\n",
    "x = layers.MaxPool2D(2)(x)\n",
    "# print(x.shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# output = [layers.Dense(1, activation='softmax', name='c%d'%(i+1))(x) for i in range(4)]\n",
    "\n",
    "output = layers.Dense(36, activation='softmax', name='output')(x)\n",
    "# print(output[-1].shape)\n",
    "\n",
    "model = Model(img_input, output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=LR),\n",
    "              metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=float32) Tensor(\"arg1:0\", shape=(), dtype=float64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    512\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    978\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(\"arg0:0\", shape=(), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2be1612039ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/zhangzhichao/github/ContentSecurity-Analyzer/ml/captcha/splited/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0miteror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2be1612039ec>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(data_dir, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mpath_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_image_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mimage_label_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_and_preprocess_from_path_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_label_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m       return DatasetV1Adapter(\n\u001b[0;32m-> 1580\u001b[0;31m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[1;32m   1581\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m       return DatasetV1Adapter(\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality)\u001b[0m\n\u001b[1;32m   2735\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 2737\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, defun_kwargs)\u001b[0m\n\u001b[1;32m   2122\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_data_structured_function_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2124\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m       \u001b[0;31m# Use the private method that will execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    488\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;31m# Adds this function into 'g'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_by_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_caller_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         whitelisted_stateful_ops=self._whitelisted_stateful_ops)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(func, arg_names, arg_types, name, capture_by_value, device, colocation_stack, container, collections_ref, arg_shapes, whitelisted_stateful_ops)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;31m# Call func and gather the output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mtf_data_structured_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2097\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2be1612039ec>\u001b[0m in \u001b[0;36m_load_and_preprocess_from_path_label\u001b[0;34m(path, label)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_and_preprocess_from_path_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_flatten_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    587\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 589\u001b[0;31m         \"ReadFile\", filename=filename, name=name)\n\u001b[0m\u001b[1;32m    590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     result = _dispatch.dispatch(\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m               raise TypeError(\"%s expected type of %s.\" %\n\u001b[0;32m--> 534\u001b[0;31m                               (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0m\u001b[1;32m    535\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m               \u001b[0;31m# Update the maps with the default, if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string."
     ]
    }
   ],
   "source": [
    "# 数据流\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from string import digits, ascii_lowercase\n",
    "import numpy as np\n",
    "import os\n",
    "CHAPTER_LIST = digits + ascii_lowercase\n",
    "\n",
    "\n",
    "def _get_file_name(path):\n",
    "    return os.path.basename(path).split('_')[0].split('.')[0]\n",
    "\n",
    "\n",
    "def _preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=1)\n",
    "    image = tf.image.resize_images(image, [60, 34])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "\n",
    "def _load_and_preprocess_from_path_label(path, label):\n",
    "    print(path, label)\n",
    "    image = tf.read_file(path)\n",
    "    return _preprocess_image(image), _flatten_labels(label)\n",
    "\n",
    "def _str2idxlist(text):\n",
    "    \"\"\"\n",
    "    text 转化为索引列表\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([_idx2onehot(CHAPTER_LIST.index(i.lower())) for i in text])\n",
    "\n",
    "\n",
    "def _idx2onehot(idx):\n",
    "    tmp = np.zeros(len(CHAPTER_LIST))\n",
    "    tmp[idx] = 1\n",
    "    return tmp\n",
    "\n",
    "def _flatten_labels(array):\n",
    "    return tf.reshape(array, (36, ))\n",
    "\n",
    "\n",
    "def get_dataset(data_dir, batch_size=50):\n",
    "    root_path = pathlib.Path(data_dir)\n",
    "    all_image_paths = [str(i) for i in root_path.glob('*.png')]\n",
    "    all_image_labels = np.array([_str2idxlist(_get_file_name(i)) for i in all_image_paths])\n",
    "\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "    image_label_ds = path_ds.map(_load_and_preprocess_from_path_label)\n",
    "\n",
    "    ds = image_label_ds.shuffle(buffer_size=100)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "    \n",
    "# dataset = get_dataset('/Users/zhangzhichao/github/ContentSecurity-Analyzer/ml/captcha/splited/train/')\n",
    "\n",
    "# iteror = dataset.make_one_shot_iterator()\n",
    "# imgs, labels = iteror.get_next()\n",
    "# print(imgs.shape)\n",
    "# print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=float32) Tensor(\"arg1:0\", shape=(), dtype=float64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    512\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    978\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(\"arg0:0\", shape=(), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2be1612039ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/zhangzhichao/github/ContentSecurity-Analyzer/ml/captcha/splited/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0miteror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2be1612039ec>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(data_dir, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mpath_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_image_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mimage_label_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_and_preprocess_from_path_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_label_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m       return DatasetV1Adapter(\n\u001b[0;32m-> 1580\u001b[0;31m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[1;32m   1581\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m       return DatasetV1Adapter(\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality)\u001b[0m\n\u001b[1;32m   2735\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 2737\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, defun_kwargs)\u001b[0m\n\u001b[1;32m   2122\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_data_structured_function_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2124\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m       \u001b[0;31m# Use the private method that will execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    488\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;31m# Adds this function into 'g'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_by_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_caller_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         whitelisted_stateful_ops=self._whitelisted_stateful_ops)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(func, arg_names, arg_types, name, capture_by_value, device, colocation_stack, container, collections_ref, arg_shapes, whitelisted_stateful_ops)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;31m# Call func and gather the output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mtf_data_structured_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2097\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2be1612039ec>\u001b[0m in \u001b[0;36m_load_and_preprocess_from_path_label\u001b[0;34m(path, label)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_and_preprocess_from_path_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_flatten_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    587\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 589\u001b[0;31m         \"ReadFile\", filename=filename, name=name)\n\u001b[0m\u001b[1;32m    590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     result = _dispatch.dispatch(\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m               raise TypeError(\"%s expected type of %s.\" %\n\u001b[0;32m--> 534\u001b[0;31m                               (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0m\u001b[1;32m    535\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m               \u001b[0;31m# Update the maps with the default, if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string."
     ]
    }
   ],
   "source": [
    "# 数据流\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from string import digits, ascii_lowercase\n",
    "import numpy as np\n",
    "import os\n",
    "CHAPTER_LIST = digits + ascii_lowercase\n",
    "\n",
    "\n",
    "def _get_file_name(path):\n",
    "    return os.path.basename(path).split('_')[0].split('.')[0]\n",
    "\n",
    "\n",
    "def _preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=1)\n",
    "    image = tf.image.resize_images(image, [60, 34])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "\n",
    "def _load_and_preprocess_from_path_label(path, label):\n",
    "    print(path, label)\n",
    "    image = tf.read_file(path)\n",
    "    return _preprocess_image(image), _flatten_labels(label)\n",
    "\n",
    "def _str2idxlist(text):\n",
    "    \"\"\"\n",
    "    text 转化为索引列表\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([_idx2onehot(CHAPTER_LIST.index(i.lower())) for i in text])\n",
    "\n",
    "\n",
    "def _idx2onehot(idx):\n",
    "    tmp = np.zeros(len(CHAPTER_LIST))\n",
    "    tmp[idx] = 1\n",
    "    return tmp\n",
    "\n",
    "def _flatten_labels(array):\n",
    "    return tf.reshape(array, (36, ))\n",
    "\n",
    "\n",
    "def get_dataset(data_dir, batch_size=50):\n",
    "    root_path = pathlib.Path(data_dir)\n",
    "    all_image_paths = [str(i) for i in root_path.glob('*.png')]\n",
    "    all_image_labels = np.array([_str2idxlist(_get_file_name(i)) for i in all_image_paths])\n",
    "\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "    image_label_ds = path_ds.map(_load_and_preprocess_from_path_label)\n",
    "\n",
    "    ds = image_label_ds.shuffle(buffer_size=100)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "    \n",
    "dataset = get_dataset('/Users/zhangzhichao/github/ContentSecurity-Analyzer/ml/captcha/splited/train/')\n",
    "\n",
    "# iteror = dataset.make_one_shot_iterator()\n",
    "# imgs, labels = iteror.get_next()\n",
    "# print(imgs.shape)\n",
    "# print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=string) Tensor(\"arg1:0\", shape=(1, 36), dtype=float64)\n",
      "Tensor(\"arg0:0\", shape=(), dtype=string) Tensor(\"arg1:0\", shape=(1, 36), dtype=float64)\n",
      "Epoch 1/30\n",
      " - 43s - loss: 0.1057 - acc: 0.9746 - val_loss: 0.9850 - val_acc: 0.8743\n",
      "Epoch 2/30\n",
      " - 43s - loss: 0.1008 - acc: 0.9757 - val_loss: 1.2335 - val_acc: 0.8758\n",
      "Epoch 3/30\n",
      " - 43s - loss: 0.0994 - acc: 0.9753 - val_loss: 1.3327 - val_acc: 0.8734\n",
      "Epoch 4/30\n",
      " - 43s - loss: 0.1030 - acc: 0.9759 - val_loss: 1.2108 - val_acc: 0.8726\n",
      "Epoch 5/30\n",
      " - 43s - loss: 0.0978 - acc: 0.9767 - val_loss: 1.1064 - val_acc: 0.8796\n",
      "Epoch 6/30\n",
      " - 43s - loss: 0.1048 - acc: 0.9752 - val_loss: 0.9478 - val_acc: 0.8819\n",
      "Epoch 7/30\n",
      " - 43s - loss: 0.1028 - acc: 0.9765 - val_loss: 1.2083 - val_acc: 0.8766\n",
      "Epoch 8/30\n",
      " - 43s - loss: 0.0996 - acc: 0.9766 - val_loss: 1.2826 - val_acc: 0.8804\n",
      "Epoch 9/30\n",
      " - 43s - loss: 0.1021 - acc: 0.9766 - val_loss: 1.1959 - val_acc: 0.8809\n",
      "Epoch 10/30\n",
      " - 43s - loss: 0.1011 - acc: 0.9773 - val_loss: 1.2912 - val_acc: 0.8768\n",
      "Epoch 11/30\n",
      " - 43s - loss: 0.1009 - acc: 0.9766 - val_loss: 1.2821 - val_acc: 0.8744\n",
      "Epoch 12/30\n",
      " - 43s - loss: 0.1008 - acc: 0.9772 - val_loss: 1.4892 - val_acc: 0.8740\n",
      "Epoch 13/30\n",
      " - 43s - loss: 0.0961 - acc: 0.9774 - val_loss: 1.3571 - val_acc: 0.8646\n",
      "Epoch 14/30\n",
      " - 43s - loss: 0.1009 - acc: 0.9770 - val_loss: 1.4427 - val_acc: 0.8754\n",
      "Epoch 15/30\n",
      " - 43s - loss: 0.1034 - acc: 0.9771 - val_loss: 1.5437 - val_acc: 0.8739\n",
      "Epoch 16/30\n",
      " - 43s - loss: 0.1024 - acc: 0.9770 - val_loss: 1.4080 - val_acc: 0.8778\n",
      "Epoch 17/30\n",
      " - 43s - loss: 0.1153 - acc: 0.9764 - val_loss: 1.3859 - val_acc: 0.8741\n",
      "Epoch 18/30\n",
      " - 44s - loss: 0.1100 - acc: 0.9771 - val_loss: 1.4933 - val_acc: 0.8756\n",
      "Epoch 19/30\n",
      " - 43s - loss: 0.1091 - acc: 0.9770 - val_loss: 1.5045 - val_acc: 0.8695\n",
      "Epoch 20/30\n",
      " - 43s - loss: 0.1018 - acc: 0.9781 - val_loss: 1.4033 - val_acc: 0.8721\n",
      "Epoch 21/30\n",
      " - 43s - loss: 0.1034 - acc: 0.9777 - val_loss: 1.3669 - val_acc: 0.8758\n",
      "Epoch 22/30\n",
      " - 43s - loss: 0.1042 - acc: 0.9777 - val_loss: 1.4326 - val_acc: 0.8774\n",
      "Epoch 23/30\n",
      " - 43s - loss: 0.1040 - acc: 0.9784 - val_loss: 1.6067 - val_acc: 0.8695\n",
      "Epoch 24/30\n",
      " - 43s - loss: 0.1083 - acc: 0.9778 - val_loss: 1.4743 - val_acc: 0.8748\n",
      "Epoch 25/30\n",
      " - 43s - loss: 0.1064 - acc: 0.9783 - val_loss: 1.4641 - val_acc: 0.8779\n",
      "Epoch 26/30\n",
      " - 43s - loss: 0.0956 - acc: 0.9796 - val_loss: 1.3139 - val_acc: 0.8778\n",
      "Epoch 27/30\n",
      " - 43s - loss: 0.0948 - acc: 0.9792 - val_loss: 1.4579 - val_acc: 0.8789\n",
      "Epoch 28/30\n",
      " - 43s - loss: 0.1007 - acc: 0.9789 - val_loss: 1.5398 - val_acc: 0.8727\n",
      "Epoch 29/30\n",
      " - 43s - loss: 0.0954 - acc: 0.9792 - val_loss: 1.5966 - val_acc: 0.8708\n",
      "Epoch 30/30\n",
      " - 43s - loss: 0.1019 - acc: 0.9786 - val_loss: 1.4778 - val_acc: 0.8792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Loss')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ce9b8c6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# train\n",
    "\n",
    "\n",
    "import datasets.base as input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_dir = '/home/moon/traindata/splited'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_dataset = get_dataset(train_dir)\n",
    "# train_iteror = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "test_dataset = get_dataset(test_dir, 2000)\n",
    "# test_iteror = test_dataset.make_one_shot_iterator()\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=1000,\n",
    "    epochs=30,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=50,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.RMSprop object at 0x7f0cea1a06d8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /home/moon/.virtualenvs/py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n"
     ]
    }
   ],
   "source": [
    "# 保存\n",
    "model.save_weights('./weights/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 34)\n",
      "(1, 60, 34, 1)\n",
      "(10000, 36)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "path = '/home/moon/traindata/splited/test/6_ebd8cef609394a908254b84ef52e63f2.png'\n",
    "# image = tf.read_file(path)\n",
    "# image = _preprocess_image(image)\n",
    "image = cv2.imread(path)\n",
    "gray_src = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "print(gray_src.shape)\n",
    "# print(image.shape)\n",
    "image = np.expand_dims(gray_src, axis=0)\n",
    "image.resize((1, 60, 34, 1))\n",
    "print(image.shape)\n",
    "ret = model.predict([image], steps=10000)\n",
    "print(ret.shape)\n",
    "\n",
    "print(CHAPTER_LIST[np.argmax(ret[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60, 34, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 58, 32, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 29, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 5, 64)         18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               328192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 36)                18468     \n",
      "=================================================================\n",
      "Total params: 369,956\n",
      "Trainable params: 369,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=string) Tensor(\"arg1:0\", shape=(1, 36), dtype=float64)\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.5734 - acc: 0.8723\n"
     ]
    }
   ],
   "source": [
    "validate_dir = '/home/moon/traindata/splited_validate/train'\n",
    "dataset = get_dataset(validate_dir)\n",
    "ret = model.evaluate(dataset, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
